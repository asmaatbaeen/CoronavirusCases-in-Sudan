{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Growth Rate Prediction\n",
    "> Predictions of COVID-19 Growth Rates Using Bayesian Modeling\n",
    "\n",
    "- comments: true\n",
    "- author: Thomas Wiecki\n",
    "- categories: [growth]\n",
    "- image: images/covid-bayesian.png\n",
    "- permalink: /growth-bayes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import arviz as az\n",
    "import pymc3 as pm\n",
    "\n",
    "import requests\n",
    "import io\n",
    "\n",
    "sns.set_context('talk')\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d630f5878fe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mdf_confirmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_timeseries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Confirmed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;31m# Drop states for simplicity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mdf_confirmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_confirmed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_confirmed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-d630f5878fe7>\u001b[0m in \u001b[0;36mload_timeseries\u001b[0;34m(name, base_url)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     df = pd.read_csv(io.StringIO(csv), \n\u001b[0;32m---> 10\u001b[0;31m                      index_col=['Country/Region', 'Province/State', 'Lat', 'Long'])\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'date'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1034\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1853\u001b[0m                 index, columns, col_dict = _get_empty_meta(\n\u001b[1;32m   1854\u001b[0m                     \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1855\u001b[0;31m                     dtype=self.kwds.get('dtype'))\n\u001b[0m\u001b[1;32m   1856\u001b[0m                 columns = self._maybe_make_multi_index_columns(\n\u001b[1;32m   1857\u001b[0m                     columns, self.col_names)\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_get_empty_meta\u001b[0;34m(columns, index_col, index_names, dtype)\u001b[0m\n\u001b[1;32m   3227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3228\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3229\u001b[0;31m             \u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3231\u001b[0m     col_dict = {col_name: Series([], dtype=dtype[col_name])\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "\n",
    "def load_timeseries(name, \n",
    "                    base_url='https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series'):\n",
    "    import requests\n",
    "    # Thanks to kasparthommen for the suggestion to directly download\n",
    "    url = f'{base_url}/time_series_19-covid-{name}.csv'\n",
    "    csv = requests.get(url).text\n",
    "    df = pd.read_csv(io.StringIO(csv), \n",
    "                     index_col=['Country/Region', 'Province/State', 'Lat', 'Long'])\n",
    "    df['type'] = name.lower()\n",
    "    df.columns.name = 'date'\n",
    "    \n",
    "    df = (df.set_index('type', append=True)\n",
    "            .reset_index(['Lat', 'Long'], drop=True)\n",
    "            .stack()\n",
    "            .reset_index()\n",
    "            .set_index('date')\n",
    "         )\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df.columns = ['country', 'state', 'type', 'cases']\n",
    "    \n",
    "    # Move HK to country level\n",
    "    df.loc[df.state =='Hong Kong', 'country'] = 'Hong Kong'\n",
    "    df.loc[df.state =='Hong Kong', 'state'] = np.nan\n",
    "    \n",
    "    # Aggregate large countries split by states\n",
    "    df = pd.concat([df, \n",
    "                    (df.loc[~df.state.isna()]\n",
    "                     .groupby(['country', 'date', 'type'])\n",
    "                     .sum()\n",
    "                     .rename(index=lambda x: x+' (total)', level=0)\n",
    "                     .reset_index(level=['country', 'type']))\n",
    "                   ])\n",
    "    return df\n",
    "\n",
    "df_confirmed = load_timeseries('Confirmed')\n",
    "# Drop states for simplicity\n",
    "df_confirmed = df_confirmed.loc[df_confirmed.state.isnull()]\n",
    "# Estimated critical cases\n",
    "p_crit = .05\n",
    "df_confirmed = df_confirmed.assign(cases_crit=df_confirmed.cases*p_crit)\n",
    "\n",
    "# Compute days relative to when 100 confirmed cases was crossed\n",
    "df_confirmed.loc[:, 'days_since_100'] = np.nan\n",
    "for country in df_confirmed.country.unique():\n",
    "    df_confirmed.loc[(df_confirmed.country == country), 'days_since_100'] = \\\n",
    "        np.arange(-len(df_confirmed.loc[(df_confirmed.country == country) & (df_confirmed.cases < 100)]), \n",
    "                  len(df_confirmed.loc[(df_confirmed.country == country) & (df_confirmed.cases >= 100)]))\n",
    "    \n",
    "# Select countries for which we have at least some information\n",
    "countries = pd.Series(df_confirmed.loc[df_confirmed.days_since_100 >= 2].country.unique())\n",
    "# We only have data for China after they already had a significant number of cases.\n",
    "# They also are not well modeled by the exponential, so we drop them here for simplicity.\n",
    "countries = countries.loc[~countries.isin(['China (total)', 'Cruise Ship (total)'])]\n",
    "df_sign = df_confirmed.loc[lambda x: x.country.isin(countries) & (x.days_since_100 >= 0)]\n",
    "n_countries = len(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the countries included in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "for c in countries:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Growth Rate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_countries' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-dd417056dbd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     a_ind = pm.Normal('a_ind', \n\u001b[1;32m     11\u001b[0m                       \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma_grp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma_grp_sigma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                       shape=n_countries)\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Slope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_countries' is not defined"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "with pm.Model() as model:\n",
    "    ############\n",
    "    # Intercept\n",
    "    # Group mean\n",
    "    a_grp = pm.Normal('a_grp', 100, 50)\n",
    "    # Group variance\n",
    "    a_grp_sigma = pm.HalfNormal('a_grp_sigma', 50)\n",
    "    # Individual intercepts\n",
    "    a_ind = pm.Normal('a_ind', \n",
    "                      mu=a_grp, sigma=a_grp_sigma, \n",
    "                      shape=n_countries)\n",
    "    ########\n",
    "    # Slope\n",
    "    # Group mean\n",
    "    b_grp = pm.Normal('b_grp', 1.33, .5)\n",
    "    # Group variance\n",
    "    b_grp_sigma = pm.HalfNormal('b_grp_sigma', .5)\n",
    "    # Individual slopes\n",
    "    b_ind = pm.Normal('b_ind', \n",
    "                      mu=b_grp, sigma=b_grp_sigma, \n",
    "                      shape=n_countries)\n",
    "    \n",
    "    # Error\n",
    "    sigma = pm.HalfNormal('sigma', 500., shape=n_countries)\n",
    "    \n",
    "    # Create likelihood for each country\n",
    "    for i, country in enumerate(countries):\n",
    "        df_country = df_sign.loc[lambda x: (x.country == country)]\n",
    "        \n",
    "        # By using pm.Data we can change these values after sampling.\n",
    "        # This allows us to extend x into the future so we can get\n",
    "        # forecasts by sampling from the posterior predictive\n",
    "        x = pm.Data(country + \"x_data\", \n",
    "                    df_country.days_since_100.values)\n",
    "        cases = pm.Data(country + \"y_data\", \n",
    "                        df_country.cases.astype('float64').values)\n",
    "        \n",
    "        # Likelihood\n",
    "        pm.NegativeBinomial(\n",
    "            country, \n",
    "            (a_ind[i] * b_ind[i] ** x), # Exponential regression\n",
    "            sigma[i], \n",
    "            observed=cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "with model:\n",
    "    # Sample posterior\n",
    "    trace = pm.sample(tune=1500, chains=1, cores=1, target_accept=.9)\n",
    "    \n",
    "    # Update data so that we get predictions into the future\n",
    "    for country in countries:\n",
    "        df_country = df_sign.loc[lambda x: (x.country == country)]\n",
    "        x_data = np.arange(0, 30)\n",
    "        y_data = np.array([np.nan] * len(x_data))\n",
    "        pm.set_data({country + \"x_data\": x_data})\n",
    "        pm.set_data({country + \"y_data\": y_data})\n",
    "    \n",
    "    # Sample posterior predictive\n",
    "    post_pred = pm.sample_posterior_predictive(trace, samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "european_countries = ['Italy', 'Germany', 'France (total)', 'Spain', 'United Kingdom (total)', \n",
    "                      'Iran']\n",
    "large_engl_countries = ['US (total)', 'Canada (total)', 'Australia (total)']\n",
    "asian_countries = ['Singapore', 'Japan', 'Korea, South', 'Hong Kong']\n",
    "south_american_countries = ['Argentina', 'Brazil', 'Colombia', 'Chile']\n",
    "\n",
    "country_groups = [european_countries, large_engl_countries, asian_countries]\n",
    "line_styles = ['-', ':', '--', '-.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "fig, axs = plt.subplots(nrows=len(country_groups), figsize=(8, 16), sharex=True)\n",
    "\n",
    "for ax, country_group in zip(axs, country_groups):\n",
    "    for i, country in enumerate(countries):\n",
    "        if country in country_group:\n",
    "            sns.distplot((trace['b_ind'][:, i] * 100) - 100, ax=ax, label=country, hist=False)\n",
    "        \n",
    "    ax.axvline(33, ls='--', color='k', label='33% daily growth')\n",
    "    ax.legend()\n",
    "ax.set_xlabel('Daily growth in %')\n",
    "plt.suptitle('Posterior of daily growth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Cases By Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "fig, axs = plt.subplots(nrows=n_countries // 3, ncols=3, figsize=(15, 30), sharex=True)\n",
    "\n",
    "for ax, country in zip(axs.flatten(), countries):\n",
    "    df_country = df_sign.loc[lambda x: x.country == country]\n",
    "    ax.plot(df_country.days_since_100, df_country.cases, color='r')\n",
    "    ax.plot(np.arange(0, post_pred[country].shape[1]), post_pred[country].T, alpha=.05, color='.5')\n",
    "    ax.plot(df_country.days_since_100, df_country.cases, color='r')\n",
    "    #ax.set_yscale('log')\n",
    "    #ax.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "    ax.set_ylim(0, df_country.cases.iloc[-1] * 15)\n",
    "    ax.set_title(country)\n",
    "    \n",
    "axs[0, 0].legend(['data', 'model prediction'])\n",
    "[ax.set(xlabel='Days since 100 cases') for ax in axs[-1, :]]\n",
    "[ax.set(ylabel='Confirmed cases') for ax in axs[:, 0]]\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Cases By Country - Log Scale\n",
    "\n",
    "Y axis is on a log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#hide_input\n",
    "fig, axs = plt.subplots(nrows=n_countries // 3, ncols=3, figsize=(15, 30), sharex=True, sharey=True)\n",
    "\n",
    "for ax, country in zip(axs.flatten(), countries):\n",
    "    df_country = df_sign.loc[lambda x: x.country == country]\n",
    "    ax.plot(df_country.days_since_100, df_country.cases, color='r')\n",
    "    ax.plot(np.arange(0, post_pred[country].shape[1]), post_pred[country].T, alpha=.05, color='.5')\n",
    "    ax.plot(df_country.days_since_100, df_country.cases, color='r')\n",
    "    ax.set_yscale('log')\n",
    "    ax.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "    ax.set_ylim(100, 1e5);\n",
    "    ax.set_title(country)\n",
    "    \n",
    "axs[0, 0].legend(['data', 'model prediction'])\n",
    "[ax.set(xlabel='Days since 100 cases') for ax in axs[-1, :]]\n",
    "[ax.set(ylabel='Confirmed cases') for ax in axs[:, 0]]\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Diagnostics - Trace Plots\n",
    "\n",
    "These are diagnostics for the model.  You can safely ignore this if not familiar with [MCMC](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "az.plot_trace(trace, compact=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About This Analysis\n",
    "\n",
    "This analysis was done by [Thomas Wiecki](https://twitter.com/twiecki)[^1]\n",
    "\n",
    "The model that we are building assumes exponential growth. This is definitely wrong because growth would just continue uninterrupted into the future. However, in the early phase of an epidemic it's a reasonable assumption.\n",
    "\n",
    "We assume a [negative binomial](https://docs.pymc.io/api/distributions/discrete.html#pymc3.distributions.discrete.NegativeBinomial) likelihood as we are dealing with count data. A Poisson could also be used but the negative binomial allows us to also model the variance separately to give more flexibility.\n",
    "\n",
    "The model is also hierarchical, pooling information from individual countries.\n",
    "\n",
    "\n",
    "[^1]:  This notebook gets up-to-date data from the [\"2019 Novel Coronavirus COVID-19 (2019-nCoV) Data Repository by Johns Hopkins CSSE\"](https://systems.jhu.edu/research/public-health/ncov/) [GitHub repository](https://github.com/CSSEGISandData/COVID-19). This code is provided under the [BSD-3 License](https://github.com/twiecki/covid19/blob/master/LICENSE). Link to [original notebook](https://github.com/twiecki/covid19/blob/master/covid19_growth_bayes.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
